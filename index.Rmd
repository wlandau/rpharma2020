---
title: "Reproducible computation at scale with targets"
author: Will Landau
output: 
  xaringan::moon_reader:
    nature:
      highlightLines: true
---

<style>
.inverse {
background-color: transparent;
text-shadow: 0 0 0px transparent;
}
.title-slide {
vertical-align: bottom !important; 
text-align: center !important;
}
.title-slide h1 {
position: absolute;
top: 0;
left: 0;
right: 0;
width: 100%;
line-height: 4em;
color: #666666;
}
.title-slide h3 {
line-height: 6em;
color: #666666;
}
.title-slide {
background-color: white;
background-image: url('images/logo.png');
background-repeat: no-repeat;
background-size: 25%;
}
.remark-slide-content:after {
content: "Copyright Eli Lilly and Company";
position: absolute;
bottom: -5px;
left: 10px;
height: 40px;
width: 100%;
font-family: Helvetica, Arial, sans-serif;
font-size: 0.7em;
color: gray;
background-repeat: no-repeat;
background-size: contain;
}
</style>

```{r, include = FALSE, message = FALSE}
library(dplyr)
library(targets)
options(
  warnPartialMatchArgs = FALSE,
  htmltools.dir.version = FALSE
)
```

```{r, include = FALSE}
knitr::opts_chunk$set(
  eval = FALSE,
  collapse = TRUE,
  warning = FALSE,
  comment = "#>",
  fig.align = "center",
  fig.width = 10,
  fig.height = 7,
  out.width = "80%",
  out.height = "80%"
)
```

## Large statistical computation

* [Bayesian data analysis](https://mc-stan.org/)
* [Bayesian network meta-analysis](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/bayesian-network-meta-analysis.html)
* [Graph-based multiple comparison procedures](https://github.com/kornl/gMCP)
* [Subgroup identification](https://cran.r-project.org/web/packages/TSDT/index.html)
* [Predictive modeling](http://appliedpredictivemodeling.com/computing)
* [Deep neural networks](https://keras.rstudio.com/)
* [PK/PD modeling](https://github.com/nlmixrdevelopment/nlmixr)
* Clinical trial simulation
* Target identification

???

In the life sciences, we develop ambitious computational workflows for Statistics and data science. There's a lot of Bayesian analysis, machine learning, etc. And we need to think about both efficiency and reproducibility right from the start.

---

## Common features

1. Heavy use of the [R language](https://www.r-project.org/).
2. Long runtimes.
3. Multiple sub-tasks.
4. Frequent changes to code and data.

<img src = "./images/sisyphus.svg" align="left" style="border: none; box-shadow: none; height: 375px; text-align: center;">
<br>

<!--https://openclipart.org/detail/275842/sisyphus-overcoming-silhouette-->

???

Many of these projects require long runtimes. Methods like MCMC and deep neural nets are computationally expensive. It could take hours or even days just to fit a single model. That's fine if you're only going to run the project once or on a nice schedule. But when the code is still under development, you run into trouble.

---

## Interconnected tasks
<center>
<img src = "./images/workflow.png" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

???

A large workflow has a large number of moving parts.

---

## Changes

<center>
<img src = "./images/change.png" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

???

If you change any one of these parts,

---

## Consequences

<center>
<img src = "./images/downstream.png" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

???

then everything that depends on it is no longer valid, and you need to rerun the computation to bring the results back up to date. This is seriously frustrating if changes happen all the time. If you're always fixing bugs or tweaking models or getting new data, then you're always trying to rerun the project, and there's no way the results can keep up...

---

## Pipeline tools and workflow managers

<center>
<img src = "./images/infographic.svg" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

- Several exist already: [github.com/pditommaso/awesome-pipeline](https://github.com/pditommaso/awesome-pipeline).
- Most are language-agnostic or designed for Python or the shell.

???

...unless you use a Make-like pipeline tool to avoid repeating yourself. There are some great tools in this space, but unfortunately not a whole lot for R.

---

## What distinguishes `targets`?

<center>
<img src = "./images/R.png" align="middle" style="border: none; box-shadow: none; text-align: center; height: 80px">
</center>

* Fundamentally designed for R.
* Supports a clean, modular, function-oriented programming style.
* Abstracts files as R objects and automatically manages data.
* Surpasses the permanent limitations of its predecessor, [`drake`](https://github.com/ropensci/drake): <https://wlandau.github.io/targets/articles/need.html>

???

That's where targets comes in. targets is a Make-like pipeline tool that is fundamentally designed for R. You can call it from an R session, it supports a function-oriented style of programming, and it abstracts away files as R objects.

---

## What about `drake`?

* `drake` is still an excellent choice for pipeline management, but it has permanent user-side limitations.
* `targets` was created to overcome these limitations and create a smoother user experience.
    1. Stronger guardrails by design.
    1. A friendlier, lighter, more transparent data management system.
    1. Show which *functions* are up to date.
    1. More flexible dynamic branching, including compatibility with `dplyr::group_by()`.
    1. Improved parallel efficiency.
    1. Designed for custom user-side [metaprogramming](https://wlandau.github.io/targets-manual/branching.html#metaprogramming) and target archetypes: <https://wlandau.github.io/tarchetypes/>.

???

But what about drake? drake has been around for a while, and it already does this. In fact, because of the user base, I'm committed to maintaining it indefinitely. But drake does have permanent user-side limitations and inefficiencies, and over four years of development, those permanent limitations have become the most urgent ones. So to advance the general capability, we need a new package. And that package is called "targets".

---

## Guardrails in `targets`

* The only way to use `targets` is the correct way.
* Main guardrails:
    1. Always run in a fresh R process (unless you deliberately configure `targets` for debugging).
    2. Require a `_targets.R` configuration file in the project root.
    3. Require the `_targets/` data store to always be in the project root.

???

First of all, targets is deliberately less flexible. Less flexibility is actually a good thing here. Targets runs the pipeline in a clean new process by default, and it nails down your working directory and data store. So it's more reproducible, more dependable, and smoother for the user.

---

## `drake`'s cache

```
.drake/
├── config/
├── data/
├───── 17bfcef645301416.rds
├───── 21935c86f12692e2.rds
├───── 37caf5df2892cfc4.rds
├───── ...
├── drake/
├── keys/
├───── memoize/
├───── meta/
├───── objects/
├───── ...
└── scratch/
```

???

Now for data management. drake's cache has a huge number of crytically-named files, which makes it difficult transfer, share, and commit to version control, not to mention the problems that come up when a file is corrupted or missing.

---

## The data store in `targets`

```
_targets/
├── meta/
├───── meta
├───── progress
├── objects/
├───── target_name_1
├───── target_name_2
├───── target_name_3
└───── ...
```

???

Targets simplifies storage. The data store is lighter, more portable, and easier to understand. It can repair itself more easily, and third-party products like Git and Dropbox and OneDrive have a much easier time.

---

## Show which functions are out of date

![](./images/graph_imports.png)

???

drake tells you which targets are out of date, but can't tell you about functions. But targets shows the status of both functions too, which makes it easier to find out why things are not up to date.

---

## Dynamic branching with `dplyr`

```{r, eval = FALSE}
library(targets)
source("functions.R") # Defines get_data() and analyze_group().
tar_options(packages = "dplyr")
tar_pipeline(
  tar_target(
    grouped_data,
    get_data() %>% group_by(id) %>% tar_group(),
    iteration = "group"
  ),
  tar_target(
    model,
    analyze_group(grouped_data),
    dynamic = map(grouped_data) # Maps over groups defined by group_by()
  )
)
```

???

Dynamic branching is more flexible. Targets lets you take a data frame, group it with dplyr, and then branch over the subsets. Folks have been asking for that in drake for years.

---

## Inefficienct dynamic branching in drake

![](./images/dynamic_drake.png)

???

Dynamic branching is also more efficient. Because of the original design of the architecture, drake treated dynamic branching with a traditional map-reduce mindset.

---

## Efficient dynamic branching in targets

![](./images/dynamic_targets.png)

???

But in targets, downstream branches can start even if some of the upstream branches are still running. So your work gets done faster.

---

## Metaprogramming

* `tar_target_raw()` avoids non-standard evaluation and supports third-party metaprogramming.
* The following are equivalent ways to define a target.

```{r, eval = FALSE}
# For most users:
tar_target(data, simulate_data(), pattern = map(index))

# For developers who metaprogram reusable pipeline archetypes:
tar_target_raw(
  "data",
  quote(simulate_data()),
  pattern = quote(map(index))
)
```

???

Lastly, targets is easier to extend and build on. There's a way to declare a target while avoiding non-standard evaluation, which opens a Pandora's Box of development opportunities at the interface level. 

---

## Target archetypes

* The `tarchetypes` package has helpers for commonly used targets: <https://wlandau.github.io/tarchetypes/>

Function | Target archetype
---|---
`tar_render()` | Render a dependency-aware R Markdown report.
`tar_knit()` | Run a dependency-aware `knitr` report.
`tar_change()` | Always run a target when a custom object changes.
`tar_force()` | Always run a target when a custom condition is true.
`tar_suppress()` | Never run a target when a custom condition is true.
`tar_plan()` | Simplified `drake`-like syntax for `targets` pipelines.

???

There's already a companion package called "tarchetypes" which uses this to support features like dependency-aware R Markdown reports.

---

## Example: COVID-19 clinical trial simulation

* Motivation: design a randomized placebo-controlled phase 2 clinical trial of a potential new treatment of COVID-19.
* Goal: understand the operating characteristics of a 200-patient trial under different effect size scenarios.
* Enroll newly hospitalized patients (within 3 days of admission) who do not need supplemental oxygen or ventilation.
* Endpoint: days until the patient is discharged from the hospital.
* Efficacy rule: graduate to phase 3 only if Prob(hazard ratio (of hospital discharge) > 1.5) > 0.6.
* Simulation:
    1. Simulate time to event data from each arm (1 treatment and 1 placebo) from normal distributions (left-truncated right-censored).
    2. Analyze with a Bayesian survival model by [Zhou, Hanson, and Zhang](https://www.jstatsoft.org/article/view/v092i09) (2020; R package [`spBayesSurv`](https://cran.r-project.org/web/packages/spBayesSurv/index.html)).
    3. Aggregate over simulations to calculate operating characteristics.


???

Let's go to an example.

I am part of a capabilities team at Lilly, and we spend a lot of time designing and simulating clinical trials. And of course this year, many of those trials focus on COVID-19. We've been using drake to manage the computation almost the entire time, and here's how it works in targets. This example is not tied to any one trial or research program in particular, it's just a simplified pedagogical example. But it does get across how we think about the computation.

---

## File structure

* Full source code: <https://github.com/EliLillyCo/targets-talk>

```{r}
run.sh
run.R
_targets.R
sge.tmpl
R/
└── functions.R
```

???

The file structure is similar. We have R scripts for functions, and an optional template file to talk to a computing cluster. The only difference here is the special _targets.R file, which I'll get to in a bit.

---

## `functions.R`

```{r}
simulate_trial <- function(
  mean_control = 15,
  mean_treatment = 10,
  patients_per_arm = 100,
  censor = 30
) {
  bind_rows(
    simulate_arm(mean_control, censor, patients_per_arm, "control"),
    simulate_arm(mean_control, censor, patients_per_arm, "treatment")
  ) %>%
    mutate(
      patients_per_arm = patients_per_arm,
      mean_control = mean_control,
      mean_treatment = mean_treatment
    )
}
```

???

Just like in drake, we're going to write functions. Each function should have no side-effects, and the return value should be a nice R object with the results. 

---

## `functions.R`

```{r}
model_hazard <- function(patients, iterations) {
  samples <- replicate(4L, run_chain(patients, iterations), simplify = FALSE)
  summarize_samples(samples, patients)
}

summarize_samples <- function(samples, patients) {
  hazard_ratio_list <- map(samples, ~as.mcmc(t(exp(.x$beta))))
  hazard_ratio <- unlist(hazard_ratio_list)
  tibble(
    prob_effect = mean(hazard_ratio > 1.5),
    median = median(hazard_ratio),
    psrf = gelman.diag(hazard_ratio_list, multivariate = FALSE)$psrf[, 1],
    patients_per_arm = patients$patients_per_arm[1],
    mean_control = patients$mean_control[1],
    mean_treatment = patients$mean_treatment[1]
  )
}

# And a few more...
```

???

Our functions usually have one of three purposes: get data, analyze data, or summarize analyses. So you're usually going to return either a dataset, a fitted model, or a table or figure. And of course, if these functions get too long, it's good practice to them up into smaller functions. And targets will be able to detect how the functions are nested.

---

## `_targets.R`

* The `_targets.R` script defines the pipeline (see `tar_script()`).

```{r}
library(targets)
source("R/functions.R")
tar_option_set(packages = c("coda", "spBayesSurv", "tidyverse", "truncnorm"))
options(clustermq.scheduler = "sge", clustermq.template = "sge.tmpl")
tar_pipeline(
  tar_target(sim, seq_len(1000), deployment = "local"),
  tar_target(mean_treatment, c(10, 20), deployment = "local"),
  tar_target(
    patients,
    simulate_trial(
      mean_control = 20,
      mean_treatment = mean_treatment,
      patients_per_arm = 100,
      censor = 30
    ),
    pattern = cross(sim, mean_treatment),
    format = "fst_tbl"
  ),
```

???

We put the pipeline together in that _targets.R file at the root of the project. Each target is a step in the computation: again, usually a dataset, analysis, or summary.

---

## `_targets.R`

```{r}
  tar_target(
    models,
    model_hazard(patients, 2000),
    pattern = map(patients),
    format = "fst_tbl"
  ),
  tar_target(
    summaries,
    summarize_models(models),
    format = "fst_tbl"
  ),
  tar_target(
    results,
    summaries,
    format = "fst_tbl",
    deployment = "local"
  )
)
```

???

A target should be large enough to take a bite out of the runtime, but small enough so that some targets can be skipped even if others need to run.

---

## Inspect the pipeline.

```{r}
tar_visnetwork()
```

![](./images/graph1.png)

???

Now before you run the pipeline, always look at the graph. Targets automatically figures out dependency relationships and execution order from static code analysis, and you want to make sure things are connected properly before you deploy a long set of jobs.


---

## Run the pipeline on a cluster.

```{r}
tar_make_clustermq(workers = 100)
#> ● run target mean_treatment
#> ● run target sim
#> ● run branch patients_db68b7ea
#> ● run branch patients_9e31afca
#> ● run branch models_eba1673a
#> ● run branch models_212ba124
#>  ...
#> ● run target summaries
#> ● run target results
```

???

We actually run the pipeline with one of the tar_make() functions. This one is set up to talk to a computing cluster through a package called clustermq. It runs the correct targets in the correct order, farms out to a bunch of remote nodes, and writes the return values to storage. All this is automatic, so your custom functions can focus on the actual methodology instead of parallel computing minutia.

---

## Inspect the results

```{r}
tar_read(results)
#> # A tibble: 2 x 6
#>   prob_success mean_treatment mean_control patients_per_arm median max_psrf
#>          <dbl>          <dbl>        <dbl>            <dbl>  <dbl>    <dbl>
#> 1        0.997             10           20              100  2.38      1.02
#> 2        0.003             20           20              100  0.996     1.02
```

???

Afterwards, all the targets are in storage. There's a special key-value store in a hidden _targets/ folder, and there are helper functions to retrieve the data. targets abstracts files as variable names. So again, less micromanagement on your end.

---

## Add a new effect size scenario in `_targets.R`

```{r}
tar_pipeline(
  # ...
  tar_target(mean_treatment, c(10, 15, 20), deployment = "local"),
  # ...
)
```

???

Now let's say we add new targets to the pipeline.

---

## Some targets are outdated.

```{r}
tar_visnetwork()
```

![](./images/graph2.png)

???

The new ones are out of date, and all the downstream ones are called into question.

---

## Only the new patients and branches run.

* Skips 2000 up-to-date models (8000 MCMC chains you do not have to run).

```{r}
tar_make_clustermq(workers = 100)
#> ✔ skip target mean_treatment
#> ✔ skip target sim
#> ✔ skip branch patients_db68b7ea
#> ✔ skip branch patients_9e31afca
#> ...
#> ● run branch patients_80f54768
#> ● run branch patients_e24a9e83
#> ...
#> ✔ skip branch models_eba1673a
#> ✔ skip branch models_212ba124
#> ...
#> ● run branch models_f324af00
#> ● run branch models_0689710c
#> ...
#> ● run target summaries
#> ● run target results
```

???

When we run the pipeline again, only the new simulation scenarios actually get computed. Anything already up to date gets skipped. In our case, that's 2000 expensive Bayesian models we don't need to run.

---

## New combined results

```{r}
readd(results)
#> # A tibble: 3 x 6
#>   prob_success mean_treatment mean_control patients_per_arm median max_psrf
#>          <dbl>          <dbl>        <dbl>            <dbl>  <dbl>    <dbl>
#> 1        0.997             10           20              100  2.38      1.02
#> 2        0.51              15           20              100  1.57      1.02
#> 3        0.003             20           20              100  0.996     1.02
```

???

And our final results reflect the new targets we added.

---

## Tangible evidence of reproducibility.

```{r}
tar_make_clustermq(workers = 100)
#> ...
#> ✔ skip target summaries
#> ✔ skip target results
#> ✓ Already up to date.
```

???

At the end of the day, the targets package can tell you if everything is up to date. This is tangible evidence that your output matches the code and data it's supposed to come from. It's evidence that someone else running the same code would get the same results. That's reproducibility. It's certainly not the only form of reproducibility, but it does increase the trust we can place in the conclusions.

---

## Links

* Development repository: <https://github.com/wlandau/targets>
* Reference website: <https://wlandau.github.io/targets/>
* User manual: <https://wlandau.github.io/targets-manual/>

## Examples

* Minimal: <https://github.com/wlandau/targets-minimal>
* Validating a Stan model: <https://github.com/wlandau/targets-stan>
* Machine learning with Keras: <https://github.com/wlandau/targets-keras>

## Helpers

* Target archetypes: <https://wlandau.github.io/tarchetypes/>
* Shiny app to help sketch pipelines: <https://wlandau.shinyapps.io/targetsketch>

???

So this was the official debut of targets. It's still a new package, and I'm about to submit it to rOpenSci and then CRAN. In the meantime, there are a bunch of ways to learn about it, and it's open source, so you can install it from GitHub and try it out.

Finally, I'd like to thank R/Pharma for the opportunity, and I'd like to thank all the early adopters of this package for the helpful feedback.

---

## References

1. Zhou, Haiming and Hanson, Timothy and Zhang, Jiajia. "spBayesSurv: Fitting Bayesian Survival Models Using R". `Journal of Statistical Software`, 92 (9), 2020. [doi:10.18637/jss.v092.i09](https://doi.org/10.18637/jss.v092.i09).
